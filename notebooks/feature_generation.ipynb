{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads audio data for augmentation\n",
    "# Borrowed from openWakeWord's automatic_model_training.ipynb, accessed March 4, 2024\n",
    "\n",
    "import datasets\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Download MIR RIR data\n",
    "\n",
    "output_dir = \"./mit_rirs\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    rir_dataset = datasets.load_dataset(\"davidscripka/MIT_environmental_impulse_responses\", split=\"train\", streaming=True)\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    for row in tqdm(rir_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1]\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "## Download noise and background audio\n",
    "\n",
    "# Audioset Dataset (https://research.google.com/audioset/dataset/index.html)\n",
    "# Download one part of the audioset .tar files, extract, and convert to 16khz\n",
    "# For full-scale training, it's recommended to download the entire dataset from\n",
    "# https://huggingface.co/datasets/agkphysics/AudioSet, and\n",
    "# even potentially combine it with other background noise datasets (e.g., FSD50k, Freesound, etc.)\n",
    "\n",
    "if not os.path.exists(\"audioset\"):\n",
    "    os.mkdir(\"audioset\")\n",
    "\n",
    "    fname = \"bal_train09.tar\"\n",
    "    out_dir = f\"audioset/{fname}\"\n",
    "    link = \"https://huggingface.co/datasets/agkphysics/AudioSet/resolve/main/data/\" + fname\n",
    "    !wget -O {out_dir} {link}\n",
    "    !cd audioset && tar -xvf bal_train09.tar\n",
    "\n",
    "    output_dir = \"./audioset_16k\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    audioset_dataset = datasets.Dataset.from_dict({\"audio\": [str(i) for i in Path(\"audioset/audio\").glob(\"**/*.flac\")]})\n",
    "    audioset_dataset = audioset_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000))\n",
    "    for row in tqdm(audioset_dataset):\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".flac\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "\n",
    "# Free Music Archive dataset\n",
    "# https://github.com/mdeff/fma\n",
    "\n",
    "output_dir = \"./fma\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    fma_dataset = datasets.load_dataset(\"rudraml/fma\", name=\"small\", split=\"train\", streaming=True)\n",
    "    fma_dataset = iter(fma_dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16000)))\n",
    "\n",
    "    # Save clips to 16-bit PCM wav files\n",
    "    n_hours = 1  # use only 1 hour of clips for this example notebook, recommend increasing for full-scale training\n",
    "    for i in tqdm(range(n_hours*3600//30)):  # this works because the FMA dataset is all 30 second clips\n",
    "        row = next(fma_dataset)\n",
    "        name = row['audio']['path'].split('/')[-1].replace(\".mp3\", \".wav\")\n",
    "        scipy.io.wavfile.write(os.path.join(output_dir, name), 16000, (row['audio']['array']*32767).astype(np.int16))\n",
    "        i += 1\n",
    "        if i == n_hours*3600//30:\n",
    "            break\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters for augmentation\n",
    "\n",
    "audio_config = {}\n",
    "\n",
    "audio_config['features_output_dir'] = 'augmented_features_mmap'\n",
    "\n",
    "audio_config['input_path'] = 'generated_samples'\n",
    "audio_config['input_glob'] = '**/*.wav'\n",
    "audio_config['impulse_paths'] = ['mit_rirs']\n",
    "audio_config['background_paths'] = ['fma', 'audioset_16k']\n",
    "audio_config['min_clip_duration_s'] = None\n",
    "audio_config['max_clip_duration_s'] = 1.39\n",
    "audio_config['max_start_time_from_right_s'] = 1.49\n",
    "audio_config['augmented_duration_s'] = 3.0\n",
    "\n",
    "from microwakeword.feature_generation import ClipsHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio clips and prepare them for augmentation\n",
    "\n",
    "clips_handler = ClipsHandler(\n",
    "                            input_path=audio_config['input_path'],\n",
    "                            input_glob=audio_config['input_glob'],\n",
    "                            impulse_paths=audio_config['impulse_paths'], \n",
    "                            background_paths=audio_config['background_paths'], \n",
    "                            augmentation_probabilities = {\n",
    "                                \"SevenBandParametricEQ\": 0.25,\n",
    "                                \"TanhDistortion\": 0.25,\n",
    "                                \"PitchShift\": 0.25,\n",
    "                                \"BandStopFilter\": 0.25,\n",
    "                                \"AddBackgroundNoise\": 0.75,\n",
    "                                \"Gain\": 1.0,\n",
    "                                \"RIR\": 0.5,\n",
    "                            },\n",
    "                            augmented_duration_s = audio_config['augmented_duration_s'],\n",
    "                            max_start_time_from_right_s = audio_config['max_start_time_from_right_s'],\n",
    "                            max_clip_duration_s = audio_config['max_clip_duration_s'],   \n",
    "                            min_clip_duration_s = audio_config['min_clip_duration_s'],     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test by playing a randomly augmented clip\n",
    "\n",
    "import IPython\n",
    "\n",
    "clips_handler.save_random_augmented_clip(\"augmented_clip.wav\")\n",
    "\n",
    "IPython.display.display(IPython.display.Audio(\"augmented_clip.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the clip's augmented features in a Ragged Mmap\n",
    "\n",
    "clips_handler.save_augmented_features(audio_config['features_output_dir'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
